{"paragraphs":[{"text":"import org.apache.spark.sql.Row\nimport org.apache.spark.mllib.regression.LabeledPoint\n//import org.apache.spark.mllib.linalg.{Vector, Vectors}\nimport org.apache.spark.ml.classification.ProbabilisticClassifier\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\nimport org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.sql.types._\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\nimport org.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegression}\n\n// Load training data\nval data = sqlContext.read.format(\"com.databricks.spark.csv\").option(\"header\", \"false\").option(\"inferSchema\",\"true\").load(\"/Users/khaslbeck/Desktop/training5.csv\")\n\n// show schema and dataset\ndata.printSchema\ndata.show\n\n// create feature vector\nval assembler =  new VectorAssembler()\n  .setInputCols(Array(\"_c2\", \"_c3\", \"_c4\"))\n  .setOutputCol(\"features\")\n  \n// assemble a new vector from dataframe\nval df1 = assembler.transform(data).select($\"_c0\".cast(DoubleType).as(\"label\"), $\"features\")\n\n// Split the data into train and test\nval splits = df1.randomSplit(Array(7.0, 3.0), seed = 1234L)\nval (train, test) = (splits(0), splits(1))\n\n// create model\nval model = new LogisticRegression()\n  .setMaxIter(700)\n  .setRegParam(0.1)\n  .setElasticNetParam(0.8)\n  .setFamily(\"binomial\")\n\n// train model\nval rModel = model.fit(train)\n\n// Print the coefficients and intercepts for logistic regression with multinomial family\nprintln(s\"Multinomial coefficients: ${rModel.coefficientMatrix}\")\nprintln(s\"Multinomial intercepts: ${rModel.interceptVector}\")\n\nval trainingSummary = rModel.summary\n\n// Obtain the objective per iteration.\nval objectiveHistory = trainingSummary.objectiveHistory\nobjectiveHistory.foreach(loss => println(loss))\n\n// Obtain the metrics useful to judge performance on test data.\n// We cast the summary to a BinaryLogisticRegressionSummary since the problem is a\n// binary classification problem.\nval binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]\n\n// Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\nval roc = binarySummary.roc\nroc.show()\nprintln(binarySummary.areaUnderROC)\n\n// Set the model threshold to maximize F-Measure\nval fMeasure = binarySummary.fMeasureByThreshold\nval maxFMeasure = fMeasure.select(max(\"F-Measure\")).head().getDouble(0)\nval bestThreshold = fMeasure.where($\"F-Measure\" === maxFMeasure)\n  .select(\"threshold\").head().getDouble(0)\nrModel.setThreshold(bestThreshold)\n\n\n//save\n//model.save(\"/user/zeppelin/CreditFraudModel\") ","user":"anonymous","dateUpdated":"2018-03-13T22:45:12-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import org.apache.spark.sql.Row\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.ml.classification.ProbabilisticClassifier\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\nimport org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.sql.types._\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\nimport org.apache.spark.ml.classification.{BinaryLogisticRegressionSummary, LogisticRegression}\ndata: org.apache.spark.sql.DataFrame = [_c0: int, _c1: int ... 3 more fields]\nroot\n |-- _c0: integer (nullable = true)\n |-- _c1: integer (nullable = true)\n |-- _c2: double (nullable = true)\n |-- _c3: double (nullable = true)\n |-- _c4: double (nullable = true)\n\n+---+-----+----+----+----+\n|_c0|  _c1| _c2| _c3| _c4|\n+---+-----+----+----+----+\n|  0|19123|0.85|2.05|0.78|\n|  0|19123|0.96|2.05|0.78|\n|  0|19123|1.09|2.05|0.78|\n|  0|19123|0.21|2.05|0.78|\n|  0|19123|0.39|2.05| 1.3|\n|  0|19123|0.08|2.44|0.24|\n|  1|19123|12.0| 2.0| 3.0|\n|  1|19123| 2.1|22.4| 3.1|\n|  1|19123| 8.0| 6.5| 9.7|\n|  0|19123|0.73|2.23|0.43|\n|  0|19123|0.77|2.23| 1.3|\n|  0|19123|1.65|2.05|0.51|\n|  0|19123|0.79|2.05|0.51|\n|  0|19123|0.61|2.05|1.54|\n|  0|19123|0.03|2.05| 1.3|\n|  0|19123|1.43|2.05|1.54|\n|  0|19123|1.37|2.05|1.26|\n|  1|19123| 1.8|15.6| 0.8|\n|  1|19123| 0.3|2.04|17.5|\n|  0|19123|1.61|2.05|0.74|\n+---+-----+----+----+----+\nonly showing top 20 rows\n\nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_5e9c70cbd251\ndf1: org.apache.spark.sql.DataFrame = [label: double, features: vector]\nsplits: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([label: double, features: vector], [label: double, features: vector])\ntrain: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\ntest: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\nmodel: org.apache.spark.ml.classification.LogisticRegression = logreg_f24e0d6c6b33\nrModel: org.apache.spark.ml.classification.LogisticRegressionModel = logreg_f24e0d6c6b33\nMultinomial coefficients: 0.18575227244787804  0.026739195005983922  0.10793384098850713  \nMultinomial intercepts: [-1.488835939950051]\ntrainingSummary: org.apache.spark.ml.classification.LogisticRegressionTrainingSummary = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary@357148ce\nobjectiveHistory: Array[Double] = Array(0.6853142072764574, 0.6639616195635891, 0.57500503936484, 0.5445884891636131, 0.5229405880289139, 0.5164224973013939, 0.5155574615210339, 0.5151951004713108, 0.515148054725231, 0.5151366564476098, 0.5151365016291882, 0.5151364959015731, 0.5151364957884423, 0.5151364957722268)\n0.6853142072764574\n0.6639616195635891\n0.57500503936484\n0.5445884891636131\n0.5229405880289139\n0.5164224973013939\n0.5155574615210339\n0.5151951004713108\n0.515148054725231\n0.5151366564476098\n0.5151365016291882\n0.5151364959015731\n0.5151364957884423\n0.5151364957722268\nbinarySummary: org.apache.spark.ml.classification.BinaryLogisticRegressionSummary = org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary@357148ce\nroc: org.apache.spark.sql.DataFrame = [FPR: double, TPR: double]\n+--------------------+--------------------+\n|                 FPR|                 TPR|\n+--------------------+--------------------+\n|                 0.0|                 0.0|\n|                 0.0|0.047619047619047616|\n|                 0.0| 0.14285714285714285|\n|                 0.0| 0.19047619047619047|\n|                 0.0|  0.2857142857142857|\n|                 0.0| 0.38095238095238093|\n|                 0.0| 0.42857142857142855|\n|                 0.0|  0.5238095238095238|\n|                 0.0|  0.6190476190476191|\n|                 0.0|  0.7142857142857143|\n|                 0.0|  0.7619047619047619|\n|                 0.0|  0.8095238095238095|\n|                 0.0|  0.8571428571428571|\n|                 0.0|  0.9047619047619048|\n|                 0.0|                 1.0|\n|0.037037037037037035|                 1.0|\n| 0.07407407407407407|                 1.0|\n|  0.1111111111111111|                 1.0|\n| 0.14814814814814814|                 1.0|\n| 0.18518518518518517|                 1.0|\n+--------------------+--------------------+\nonly showing top 20 rows\n\n1.0\nfMeasure: org.apache.spark.sql.DataFrame = [threshold: double, F-Measure: double]\nmaxFMeasure: Double = 1.0\nbestThreshold: Double = 0.3427634737856114\nres671: rModel.type = logreg_f24e0d6c6b33\n"}]},"apps":[],"jobName":"paragraph_1520989736581_-1413016038","id":"20160422-130535_1047679357","dateCreated":"2018-03-13T21:08:56-0400","dateStarted":"2018-03-13T22:45:12-0400","dateFinished":"2018-03-13T22:45:20-0400","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4307"},{"user":"anonymous","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520992641291_-63352479","id":"20180313-215721_741898231","dateCreated":"2018-03-13T21:57:21-0400","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:4979","text":"spark.version","dateUpdated":"2018-03-13T21:57:28-0400","dateFinished":"2018-03-13T21:57:28-0400","dateStarted":"2018-03-13T21:57:28-0400","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res96: String = 2.1.0\n"}]}},{"text":"//val model =  org.apache.spark.ml.classification.LogisticRegressionModel.load( \"/user/zeppelin/ccModel6\")\n\nprintln(s\"Weights: ${model.weights} Intercept: ${model.intercept} ${model.coefficients}\")\n\n//model.summary\n\n//val trainingSummary = model.summary\n\n// Obtain the objective per iteration.\nval objectiveHistory = trainingSummary.objectiveHistory\nobjectiveHistory.foreach(loss => println(loss))\n\n//val tran = sqlc.createDataFrame(Seq((1.0, Vectors.dense( 1.8, 15.6, 0.8)))).toDF(\"label\", \"features\")\n    \nval tran = Seq(\n    LabeledPoint(1.0, Vectors.dense(5.9,7.55,2.3) ),\n    LabeledPoint(0.0, Vectors.dense(0.9,1.55,1.3) ),\n    LabeledPoint(1.0, Vectors.dense(14.0,23.55,34.3) )\n    ).toDF\n\nval prediction = model.transform(tran)\n\nprediction.select(\"label\", \"prediction\",\"features\", \"probability\").show(3)    \nprediction.select(\"label\", \"prediction\",\"features\", \"probability\", \"rawPrediction\").collect.foreach(println)","dateUpdated":"2018-03-13T22:44:03-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true,"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"<console>:389: error: value weights is not a member of org.apache.spark.ml.classification.LogisticRegression\n       println(s\"Weights: ${model.weights} Intercept: ${model.intercept} ${model.coefficients}\")\n                                  ^\n<console>:389: error: value intercept is not a member of org.apache.spark.ml.classification.LogisticRegression\n       println(s\"Weights: ${model.weights} Intercept: ${model.intercept} ${model.coefficients}\")\n                                                              ^\n<console>:389: error: value coefficients is not a member of org.apache.spark.ml.classification.LogisticRegression\n       println(s\"Weights: ${model.weights} Intercept: ${model.intercept} ${model.coefficients}\")\n                                                                                 ^\n"}]},"apps":[],"jobName":"paragraph_1520989736582_-1411861792","id":"20160422-130559_1499050426","dateCreated":"2018-03-13T21:08:56-0400","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:4308","user":"anonymous","dateFinished":"2018-03-13T22:44:04-0400","dateStarted":"2018-03-13T22:44:03-0400"},{"text":"%sql\nselect * from row","dateUpdated":"2018-03-13T21:08:56-0400","config":{"colWidth":12,"editorMode":"ace/mode/sql","results":[{"graph":{"mode":"stackedAreaChart","height":300,"optionOpen":false,"keys":[{"name":"_1","index":0,"aggr":"sum"},{"name":"_2","index":1,"aggr":"sum"},{"name":"_3","index":2,"aggr":"sum"},{"name":"_4","index":3,"aggr":"sum"}],"values":[{"name":"_1","index":0,"aggr":"sum"},{"name":"_2","index":1,"aggr":"sum"},{"name":"_3","index":2,"aggr":"sum"},{"name":"_4","index":3,"aggr":"sum"}],"groups":[],"scatter":{"xAxis":{"name":"_1","index":0,"aggr":"sum"},"yAxis":{"name":"_2","index":1,"aggr":"sum"}}}}],"enabled":true,"editorSetting":{"language":"sql"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"_1\t_2\t_3\t_4\n0.0\t0.96\t2.05\t0.78\n0.0\t1.09\t2.05\t0.78\n0.0\t0.21\t2.05\t0.78\n0.0\t0.39\t2.05\t1.3\n0.0\t0.08\t2.44\t0.24\n1.0\t12.0\t2.0\t3.0\n1.0\t2.1\t22.4\t3.1\n1.0\t8.0\t6.5\t9.7\n0.0\t0.73\t2.23\t0.43\n0.0\t0.77\t2.23\t1.3\n0.0\t1.65\t2.05\t0.51\n0.0\t0.79\t2.05\t0.51\n0.0\t0.61\t2.05\t1.54\n0.0\t0.03\t2.05\t1.3\n0.0\t1.43\t2.05\t1.54\n0.0\t1.37\t2.05\t1.26\n1.0\t1.8\t15.6\t0.8\n1.0\t0.3\t2.04\t17.5\n0.0\t1.61\t2.05\t0.74\n0.0\t1.31\t2.05\t1.3\n0.0\t1.32\t2.05\t0.51\n0.0\t0.95\t2.05\t0.51\n0.0\t1.01\t2.05\t1.54\n0.0\t0.73\t2.44\t1.54\n0.0\t0.06\t2.23\t1.12\n0.0\t0.99\t2.23\t0.04\n0.0\t1.65\t2.05\t1.22\n0.0\t0.09\t2.05\t1.54\n0.0\t1.56\t2.05\t0.24\n0.0\t1.46\t2.05\t1.3\n0.0\t1.57\t2.05\t0.74\n0.0\t0.79\t2.05\t1.22\n0.0\t0.96\t2.05\t0.78\n0.0\t0.06\t2.05\t1.06\n0.0\t0.78\t2.05\t1.22\n0.0\t0.72\t2.05\t1.22\n0.0\t1.9\t2.05\t0.04\n0.0\t0.3\t2.05\t1.54\n0.0\t0.5\t2.05\t1.26\n1.0\t12.0\t2.0\t3.0\n1.0\t2.1\t22.4\t3.1\n1.0\t8.0\t6.5\t9.7\n1.0\t5.9\t2.0\t33.3\n1.0\t14.0\t3.3\t4.1\n1.0\t1.4\t15.7\t2.0\n1.0\t7.2\t9.5\t11.2\n1.0\t0.9\t1.1\t16.6\n1.0\t11.4\t0.9\t1.3\n1.0\t1.8\t15.6\t0.8\n1.0\t0.3\t2.04\t10.5\n1.0\t22.0\t2.0\t3.0\n1.0\t2.1\t22.4\t3.1\n1.0\t8.0\t67.5\t19.7\n1.0\t5.9\t2.0\t33.3\n1.0\t14.0\t3.3\t4.1\n1.0\t1.4\t15.7\t2.0\n1.0\t7.2\t9.5\t11.2\n1.0\t0.9\t1.1\t16.6\n1.0\t11.4\t0.9\t1.3\n1.0\t1.8\t15.6\t0.8\n1.0\t0.3\t2.04\t17.5\n"}]},"apps":[],"jobName":"paragraph_1520989736582_-1411861792","id":"20160422-132731_193522292","dateCreated":"2018-03-13T21:08:56-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4309"},{"dateUpdated":"2018-03-13T21:08:56-0400","config":{"colWidth":12,"editorMode":"ace/mode/scala","graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"results":{},"editorSetting":{"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1520989736583_-1412246541","id":"20160422-134041_1986121581","dateCreated":"2018-03-13T21:08:56-0400","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:4310"}],"name":"Credit Fraud Detection ML","id":"2DAMKEJ9Y","angularObjects":{"2D7UB5DWN:shared_process":[],"2D77X3KB6:shared_process":[],"2D78CY7V1:shared_process":[],"2D9TR66MS:shared_process":[],"2D8A6P2VJ:shared_process":[],"2D763B6N4:shared_process":[],"2D8XDFXUS:shared_process":[],"2D9YUB6X9:shared_process":[],"2D89XEJVA:shared_process":[],"2D9TNV8XZ:shared_process":[],"2D9HGKTCK:shared_process":[],"2D9ECNFF2:shared_process":[],"2D6CZP8YB:shared_process":[],"2D92BFGM4:shared_process":[],"2D9KC5YRP:shared_process":[],"2D99TYCWV:shared_process":[],"2D8DWKU88:shared_process":[],"2D7YQZDKH:shared_process":[],"2D9TZV2HZ:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}